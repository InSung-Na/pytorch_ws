{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensor 다루기 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 Tensor type이 Variable과 Constant를 포함함\n",
    "\n",
    "Tensorflow에는 Variable 타입이 있었고, 이는 자동미분을 위한 기능이었는데, 이 기능을 Tensor타입에 포함시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 형변환"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list, array -> Torch.tensor\n",
    "- torch.tensor() : 기존 데이터의 복사본\n",
    "- torch.as_tensor() : 기존 데이터를 공유하는 텐서\n",
    "- torch.from_numpy() : 기존 numpy 배열을 공유하는 텐서(numpy전용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "li = [[1, 2,], [3,4]]\n",
    "\n",
    "li_tensor = torch.tensor(li)\n",
    "# li_as_tensor = torch.as_tensor(li)\n",
    "\n",
    "print(li_tensor)\n",
    "print(li_tensor.shape)\n",
    "print(li_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "torch.Size([2, 2])\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1,2],[3,4]])\n",
    "arr_tensor = torch.tensor(arr)\n",
    "# arr_as_tensor = torch.as_tensor(arr)\n",
    "# arr_from_numpy = torch.from_numpy(arr)\n",
    "\n",
    "print(arr_tensor)\n",
    "print(arr_tensor.shape) # tensor.size() 같은 결과\n",
    "print(arr_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch -> numpy.arr\n",
    "- tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(li_tensor.numpy())\n",
    "print(arr_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 형변환\n",
    "- var = tensor.type(torch.TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(li_tensor.type(torch.float64))\n",
    "print(arr_tensor.type(torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특정한 값의 Tensor 생성하기\n",
    "- torch.arange()\n",
    "- torch.ones(), torch.zeros()\n",
    "- torch.ones_like(), torch.zeros_like()\n",
    "- torch.linspace()\n",
    "- torch.logspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)    # range()와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, 3), torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]), tensor([0., 0., 0.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2), torch.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4]]),\n",
       " tensor([[1, 1],\n",
       "         [1, 1]]),\n",
       " tensor([[0, 0],\n",
       "         [0, 0]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당 크기만큼 특정 값으로 배열생성\n",
    "li_tensor, torch.ones_like(li_tensor), torch.zeros_like(li_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e+02, 1.0000e+05, 3.1623e+07, 1.0000e+10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(0, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 난수 생성하기\n",
    "- torch.rand() : 균등분포\n",
    "- torch.randn() : 정규분포\n",
    "- torch.randint(n, size=(,)) : 양의 정수 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dc5ba50230>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5326, 0.9955, 0.4764, 0.6533, 0.1295])\n",
      "tensor([-0.2342,  1.4572,  0.0635,  1.0819,  0.5838])\n",
      "tensor([3, 3, 6, 9, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5)\n",
    "b = torch.randn(5)\n",
    "c = torch.randint(10, size=(5,))\n",
    "print(a, b, c, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용하기\n",
    "- Torch에서는 GPU사용을 위해서 TypeCasting 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda 사용가능 여부\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 를 사용하기 위해 Cuda에서 사용하는 데이터타입으로 바꾸어줘야 한다. \n",
    "\n",
    "방법 세가지! \n",
    "\n",
    " - 만들 때, device 설정해두기\n",
    " - tensor_var.cuda()\n",
    " - tensor_var.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 설정하기.\n",
    "x = torch.ones(2, 2, device='cuda')\n",
    "\n",
    "# 여러개 중에 하나의 GPU에 할당하고 싶을 때\n",
    "# 번호는 nvidia-smi 명령을 Shell에 입력해서 찾을 수 있음 \n",
    "x = torch.ones(2, 2, device='cuda:0')\n",
    "\n",
    "# device 객체를 입력하는게 기본\n",
    "x = torch.ones(2, 2, device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .cuda()\n",
    "a = torch.rand(10)\n",
    "print(a)\n",
    "\n",
    "a = a.cuda()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .to(device)\n",
    "a = torch.rand(2)\n",
    "print(a)\n",
    "\n",
    "a = a.to(\"cuda\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2)\n",
    "print(a)\n",
    "\n",
    "a = a.to(torch.device(\"cuda\"))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분 그냥 이렇게 사용합니다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([0.0463, 0.7543])\n",
      "tensor([0.0463, 0.7543])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "a = torch.rand(2)\n",
    "print(a)\n",
    "\n",
    "a = a.to(device)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensor Calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.add()\n",
    "- torch.sub()\n",
    "- torch.mul()\n",
    "- torch.div()\n",
    "- torch.pow()\n",
    "- torch.negative()\n",
    "- 그 외 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0, 5)\n",
    "y = torch.arange(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1, -1, -1, -1, -1])\n",
      "tensor([-1, -1, -1, -1, -1])\n",
      "tensor([-1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "print(x.sub(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  2,  6, 12, 20])\n",
      "tensor([ 0,  2,  6, 12, 20])\n",
      "tensor([ 0,  2,  6, 12, 20])\n"
     ]
    }
   ],
   "source": [
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "print(x.mul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.5000, 0.6667, 0.7500, 0.8000])\n",
      "tensor([0.0000, 0.5000, 0.6667, 0.7500, 0.8000])\n",
      "tensor([0.0000, 0.5000, 0.6667, 0.7500, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "print(x.div(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    1,    8,   81, 1024])\n",
      "tensor([   0,    1,    8,   81, 1024])\n",
      "tensor([   0,    1,    8,   81, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(x**y)\n",
    "print(torch.pow(x, y))\n",
    "print(x.pow(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, -1, -2, -3, -4])\n",
      "tensor([ 0, -1, -2, -3, -4])\n",
      "tensor([ 0, -1, -2, -3, -4])\n"
     ]
    }
   ],
   "source": [
    "print(-x)\n",
    "print(torch.negative(x))\n",
    "print(x.negative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.abs`: 절대값\n",
    "* `torch.sign`: 부호\n",
    "* `torch.round`: 반올림\n",
    "* `torch.ceil`: 올림\n",
    "* `torch.floor`: 내림\n",
    "* `torch.square`: 제곱\n",
    "* `torch.sqrt`: 제곱근\n",
    "* `torch.maximum`: 두 텐서의 각 원소에서 최댓값만 반환.\n",
    "* `torch.minimum`: 두 텐서의 각 원소에서 최솟값만 반환.\n",
    "* `torch.cumsum`: 누적합\n",
    "* `torch.cumprod`: 누적곱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0463, 0.7543])\n",
      "<built-in method sign of type object at 0x00007FFED642C560>\n",
      "<built-in method round of type object at 0x00007FFED642C560>\n",
      "<built-in method ceil of type object at 0x00007FFED642C560>\n",
      "<built-in method floor of type object at 0x00007FFED642C560>\n",
      "<built-in method square of type object at 0x00007FFED642C560>\n",
      "<built-in method sqrt of type object at 0x00007FFED642C560>\n",
      "<built-in method maximum of type object at 0x00007FFED642C560>\n",
      "<built-in method minimum of type object at 0x00007FFED642C560>\n",
      "<built-in method cumsum of type object at 0x00007FFED642C560>\n",
      "<built-in method cumprod of type object at 0x00007FFED642C560>\n"
     ]
    }
   ],
   "source": [
    "print(torch.abs)\n",
    "print(torch.sign)\n",
    "print(torch.round)\n",
    "print(torch.ceil)\n",
    "print(torch.floor)\n",
    "print(torch.square)\n",
    "print(torch.sqrt)\n",
    "print(torch.maximum)\n",
    "print(torch.minimum)\n",
    "print(torch.cumsum)\n",
    "print(torch.cumprod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원 축소 연산"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch는 기본이 reduce 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 4])\n",
      "tensor([[[2, 0, 2, 3],\n",
      "         [5, 9, 8, 5]],\n",
      "\n",
      "        [[6, 9, 5, 8],\n",
      "         [2, 7, 9, 8]],\n",
      "\n",
      "        [[5, 7, 6, 8],\n",
      "         [6, 7, 4, 0]],\n",
      "\n",
      "        [[6, 9, 0, 2],\n",
      "         [5, 2, 0, 7]],\n",
      "\n",
      "        [[6, 4, 8, 2],\n",
      "         [1, 3, 1, 3]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(10, size=(5, 2, 4))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if dim=0\n",
    "  (2,4)를 유지하기 위해 각 그룹의 인덱스 기준 요소끼리 sum\n",
    "if dim=1\n",
    "  (5,4)를 만들기 위해 각 그룹의 행을 축소\n",
    "if dim=2\n",
    "  (5,2)를 만들기 위해 칼럼 축소\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(190)\n",
      "torch.Size([])\n",
      "tensor([[25, 29, 21, 23],\n",
      "        [19, 28, 22, 23]])\n",
      "torch.Size([2, 4])\n",
      "tensor([[ 7,  9, 10,  8],\n",
      "        [ 8, 16, 14, 16],\n",
      "        [11, 14, 10,  8],\n",
      "        [11, 11,  0,  9],\n",
      "        [ 7,  7,  9,  5]])\n",
      "torch.Size([5, 4])\n",
      "tensor([[ 7, 27],\n",
      "        [28, 26],\n",
      "        [26, 17],\n",
      "        [17, 14],\n",
      "        [20,  8]])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(x))\n",
    "print(torch.sum(x).shape)\n",
    "print(torch.sum(x, dim=0))\n",
    "print(torch.sum(x, dim=0).shape)\n",
    "print(torch.sum(x, dim=1))\n",
    "print(torch.sum(x, dim=1).shape)\n",
    "print(torch.sum(x, dim=2))\n",
    "print(torch.sum(x, dim=2).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 0.],\n",
       "         [0., 1.]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2, 0], [0, 1]], dtype=torch.float32)\n",
    "b = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b) # torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.0000],\n",
       "        [0.0000, 1.0000]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.inv(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크기와 차원을 바꾸는 명령 \n",
    " - `torch.reshape`\n",
    " - `.view`\n",
    " - `torch.transpose`\n",
    "\n",
    " #### 차원 다루기\n",
    " - `torch.squeeze()`\n",
    " - `torch.unsqueeze()` = `tensor.expand_dims()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view 함수는 이렇게 못씀\n",
    "# torch.view(a, (4, -1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "tensor([[2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "torch.Size([4, 1]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(a.view(4, -1), a.reshape((4, -1)), sep=\"\\n\")\n",
    "print(a.view(4, -1).shape, a.reshape((4, -1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(a, (4, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]]]),\n",
       " torch.Size([1, 4, 1]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view함수가 있어 expand_dims 같은 함수가 따로 필요없다. \n",
    "a.view((1, 4, 1)), a.view((1, 4, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0., 0., 1.]])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.squeeze(a.view((1, 4, 1)), dim=2))\n",
    "print(torch.squeeze(a.view((1, 4, 1)), dim=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand_dims 함수 대신에 unsqueeze 함수가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]])\n",
      "torch.Size([1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.unsqueeze(a.view((1, 4)), dim=2))\n",
    "print(torch.unsqueeze(a.view((1, 4)), dim=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 여러 함수가 torch.명령어 형태, tensor.명령어 로  사용 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).view(5, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4, 6, 8],\n",
       "        [1, 3, 5, 7, 9]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4, 6, 8],\n",
       "        [1, 3, 5, 7, 9]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(a, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 끝에 `_`를 붙이면 inplace명령이 된다.(안되는 함수가 존재함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "tensor([[0, 2, 4, 6, 8],\n",
      "        [1, 3, 5, 7, 9]])\n",
      "tensor([[0, 2, 4, 6, 8],\n",
      "        [1, 3, 5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.transpose_(0, 1))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transpose 하고 view  했을 때 이런 에러가 발생하기도 한다. \n",
    "\n",
    "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
    "\n",
    "\n",
    "이는 PyTorch가 데이터를 메모리에 저장하는 방식과 view, reshape, transpose 연산들이 연산을 수행하는 방식의 차이에 기인한다. \n",
    "메모리상의 데이터의 물리적 위치와 index가 일치 할 때 contiguous 하다고 표현하는데, view는 contiguous해야만 연산을 수행할 수 있다. \n",
    "\n",
    "이를 해결하기위해 tensor.contiguous() 함수를 호출하여 데이터를 정리해주면 된다. 근데! 이런 이슈는 가끔 나오는 거니 어려우면 지금은 그냥 넘어가도 좋다!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = a.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a.view(-1, 1), a.view(-1, 1).shape)\n",
    "# print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 구성 동일하게 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(a.transpose_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(10).view(2, 5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(a.view_as(b))\n",
    "print(a.reshape_as(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.contiguous()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing, slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **텐서를 나누거나 두 개 이상의 텐서를 합치는 명령**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나누는 연산\n",
    "- `torch.chunk(data, num, dim=N)` : N차원을 num으로 나눈다\n",
    "- `torch.split(data, num, dim=N)` : N차원을 num개로 분할한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 6]),\n",
       " tensor([[0.1480, 0.8334, 0.0741, 0.7204, 0.9621, 0.9116],\n",
       "         [0.3474, 0.8684, 0.0410, 0.9469, 0.8626, 0.5865],\n",
       "         [0.4435, 0.9634, 0.4417, 0.4879, 0.0600, 0.8219]]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.rand(3, 6)\n",
    "c.shape, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1480, 0.8334],\n",
      "        [0.3474, 0.8684],\n",
      "        [0.4435, 0.9634]])\n",
      "tensor([[0.0741, 0.7204],\n",
      "        [0.0410, 0.9469],\n",
      "        [0.4417, 0.4879]])\n",
      "tensor([[0.9621, 0.9116],\n",
      "        [0.8626, 0.5865],\n",
      "        [0.0600, 0.8219]])\n"
     ]
    }
   ],
   "source": [
    "c1, c2, c3 = torch.chunk(c, 3, dim=1)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1480, 0.8334, 0.0741, 0.7204, 0.9621, 0.9116]])\n",
      "tensor([[0.3474, 0.8684, 0.0410, 0.9469, 0.8626, 0.5865]])\n",
      "tensor([[0.4435, 0.9634, 0.4417, 0.4879, 0.0600, 0.8219]])\n"
     ]
    }
   ],
   "source": [
    "c1, c2, c3 = torch.chunk(c, 3, dim=0)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1480, 0.8334, 0.0741, 0.7204, 0.9621, 0.9116]]),\n",
       " tensor([[0.3474, 0.8684, 0.0410, 0.9469, 0.8626, 0.5865]]),\n",
       " tensor([[0.4435, 0.9634, 0.4417, 0.4879, 0.0600, 0.8219]]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(c, 1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1480, 0.8334],\n",
       "         [0.3474, 0.8684],\n",
       "         [0.4435, 0.9634]]),\n",
       " tensor([[0.0741, 0.7204],\n",
       "         [0.0410, 0.9469],\n",
       "         [0.4417, 0.4879]]),\n",
       " tensor([[0.9621, 0.9116],\n",
       "         [0.8626, 0.5865],\n",
       "         [0.0600, 0.8219]]))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(c, 2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1480, 0.8334, 0.0741],\n",
       "         [0.3474, 0.8684, 0.0410],\n",
       "         [0.4435, 0.9634, 0.4417]]),\n",
       " tensor([[0.7204, 0.9621, 0.9116],\n",
       "         [0.9469, 0.8626, 0.5865],\n",
       "         [0.4879, 0.0600, 0.8219]]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(c, 3, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결합하는 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(3, 3)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[237], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mcat([a, b], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "torch.cat([a, b], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones_like(b)\n",
    "a, a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [0., 0., 0.]]]),\n",
       " torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b], dim=1), torch.stack([a, b], dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]),\n",
       " torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b], dim=0), torch.stack([a, b], dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]]]),\n",
       " torch.Size([3, 3, 2]))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b], dim=2), torch.stack([a, b], dim=2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tile(a, (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tile(a, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.vstack>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.hstack>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 자동 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd`는 PyTorch에서 핵심적인 기능을 담당하는 하부 패키지이다. \n",
    "\n",
    "autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능을 한다.\n",
    "\n",
    "- 텐서 자료를 생성할 때, `requires_grad`인수를 `True`로 설정하거나 \n",
    "- `.requires_grad_(True)`를 실행하면\n",
    "\n",
    "그 텐서에 행해지는 모든 연산에 대한 미분값을 계산한다. \n",
    "\n",
    "계산을 멈추고 싶으면 `.detach()`함수나 with을 이용해 `torch.no_grad()`를  이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 이 x에 연산을 수행한다. 다음 코드의 y는 연산의 결과이므로 미분 함수를 가진다. `grad_fn`속성을 출력해 미분 함수를 확인 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sum(x * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7438, grad_fn=<SumBackward0>) <SumBackward0 object at 0x000001DC6251C880>\n"
     ]
    }
   ],
   "source": [
    "print(y, y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y.backward()` 함수를 실행하면 x의 미분값이 자동으로 갱신된다. x의 `grad`속성을 확인하면 미분값이 들어 있는 것을 확인 할 수 있다. y를 구하기 위한 x의 연산을 수식으로 쓰면 다음과 같다. \n",
    "\n",
    "$$\n",
    "y = \\displaystyle\\sum_{i=1}^4 3 \\times x_i\n",
    "$$\n",
    "\n",
    "이를 $x_i$에 대해 미분 하면 미분 함수는 다음과 같다. \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial y}{\\partial x_i} = 3\n",
    "$$\n",
    "\n",
    "실제 미분값과 같은지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`함수는 자동으로 미분값을 계산해 `requires_grad`인수가 True로 설정된 변수의 `grad`속성의 값을 갱신한다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retain_graph` 미분을 연산하기 위해서 사용했던 임시 그래프를 유지 할 것인가를 설정하는 것이다. \n",
    "기본값은 False로 설정되어 있지만 동일한 연산에 대해 여러번 미분을 계산하기 위해서는 True로 설정되어 있어야한다.(`tf.GradientTape`에서 `persistent`와 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sum(x * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`torch.autograd.grad()`함수를 사용해 `tf.GradientTape`처럼 사용할 수도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3.],\n",
       "         [3., 3.]]),)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상황에 따라 특정 연산에는 미분값을 계산하고 싶지 않은 경우에는 \n",
    "\n",
    " - `.detach()`함수\n",
    " - `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5193, 0.6567],\n",
       "        [0.6080, 0.5344]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.grad)\n",
    "x_d = x.detach()\n",
    "torch.sigmoid(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x_d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_d2 = torch.sigmoid(x)\n",
    "    print(x_d2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PyTorch 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dc5ba50230>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>3.47428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718</td>\n",
       "      <td>8.780</td>\n",
       "      <td>82.9</td>\n",
       "      <td>1.9047</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>354.55</td>\n",
       "      <td>5.29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.07896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "      <td>6.273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2515</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.92</td>\n",
       "      <td>6.78</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1.83377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>7.802</td>\n",
       "      <td>98.2</td>\n",
       "      <td>2.0407</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>389.61</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2.92400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>6.101</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.2834</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>240.16</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX   \n",
       "501  3.47428  0.0  18.10   1.0  0.718  8.780  82.9  1.9047  24.0  666.0  \\\n",
       "502  0.07896  0.0  12.83   0.0  0.437  6.273   6.0  4.2515   5.0  398.0   \n",
       "503  1.83377  0.0  19.58   1.0  0.605  7.802  98.2  2.0407   5.0  403.0   \n",
       "504  0.35809  0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "505  2.92400  0.0  19.58   0.0  0.605  6.101  93.0  2.2834   5.0  403.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  const  \n",
       "501     20.2  354.55   5.29    1.0  \n",
       "502     18.7  394.92   6.78    1.0  \n",
       "503     14.7  389.61   1.92    1.0  \n",
       "504     17.4  391.70   9.71    1.0  \n",
       "505     14.7  240.16   9.81    1.0  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "# Load the data\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# Combine the training and test data\n",
    "data = np.concatenate((train_data, test_data), axis=0)\n",
    "targets = np.concatenate((train_targets, test_targets), axis=0)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "df['const'] = np.ones(df.shape[0])\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(df.values)\n",
    "y = torch.tensor(targets).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([506, 14]), torch.Size([506, 1]))"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이론으로 풀이\n",
    "- 최적의 가중치 $w = (X^T X)^(-1)  X^T y$를 구한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 506])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XT = torch.transpose(x, 0, 1)\n",
    "XT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0801e-01],\n",
       "        [ 4.6420e-02],\n",
       "        [ 2.0559e-02],\n",
       "        [ 2.6867e+00],\n",
       "        [-1.7767e+01],\n",
       "        [ 3.8099e+00],\n",
       "        [ 6.9222e-04],\n",
       "        [-1.4756e+00],\n",
       "        [ 3.0605e-01],\n",
       "        [-1.2335e-02],\n",
       "        [-9.5275e-01],\n",
       "        [ 9.3117e-03],\n",
       "        [-5.2476e-01],\n",
       "        [ 3.6459e+01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.mm(torch.mm(torch.linalg.inv(torch.mm(XT, x)), XT), y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.mm(x, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand((14, 1), dtype=torch.float64, requires_grad=True)\n",
    "b = torch.rand((1, 1), dtype=torch.float64, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.mm(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((z - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.9010e+03],\n",
       "         [9.9108e+03],\n",
       "         [1.2561e+04],\n",
       "         [6.8508e+01],\n",
       "         [5.8227e+02],\n",
       "         [6.3159e+03],\n",
       "         [7.3607e+04],\n",
       "         [3.5648e+03],\n",
       "         [1.1625e+04],\n",
       "         [4.5606e+05],\n",
       "         [1.8969e+04],\n",
       "         [3.5656e+05],\n",
       "         [1.3858e+04],\n",
       "         [1.0141e+03]], dtype=torch.float64),\n",
       " tensor([[1014.0671]], dtype=torch.float64))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss.numpy()는 오류발생\n",
    "```\n",
    "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "로그를 남기기 위해서는 detach().numpy()를 사용해달라고 안내하고 있다\n",
    "```\n",
    "\n",
    "동일기능\n",
    "```\n",
    "torch.no_grad()안에서는 미분이 수행이 안되므로 torch.numpy()가능\n",
    "with torch.no_grad():\n",
    "    print(loss.numpy())\n",
    "```\n",
    "\n",
    "더 간단한 방법\n",
    "```\n",
    "loss.item()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303557.167825324\n",
      "303557.167825324\n"
     ]
    }
   ],
   "source": [
    "print(loss.detach().numpy())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign 대신에 data에 접근해서 값을 수정 \n",
    "\n",
    "tensor.data = 다른데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand((14, 1), dtype=torch.float64, requires_grad=True)\n",
    "b = torch.rand((1, 1), dtype=torch.float64, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - loss : 239.3950063292629\n",
      "1 - loss : 239.3273477402613\n",
      "2 - loss : 239.25971924707068\n",
      "3 - loss : 239.19212083355347\n",
      "4 - loss : 239.12455248358262\n",
      "5 - loss : 239.05701418104198\n",
      "6 - loss : 238.98950590982602\n",
      "7 - loss : 238.92202765384013\n",
      "8 - loss : 238.8545793970003\n",
      "9 - loss : 238.7871611232332\n",
      "10 - loss : 238.71977281647628\n",
      "11 - loss : 238.65241446067773\n",
      "12 - loss : 238.58508603979618\n",
      "13 - loss : 238.51778753780118\n",
      "14 - loss : 238.45051893867307\n",
      "15 - loss : 238.38328022640252\n",
      "16 - loss : 238.31607138499095\n",
      "17 - loss : 238.24889239845064\n",
      "18 - loss : 238.18174325080415\n",
      "19 - loss : 238.11462392608502\n",
      "20 - loss : 238.0475344083372\n",
      "21 - loss : 237.98047468161545\n",
      "22 - loss : 237.91344472998475\n",
      "23 - loss : 237.84644453752108\n",
      "24 - loss : 237.77947408831074\n",
      "25 - loss : 237.7125333664508\n",
      "26 - loss : 237.64562235604876\n",
      "27 - loss : 237.57874104122274\n",
      "28 - loss : 237.51188940610137\n",
      "29 - loss : 237.44506743482387\n",
      "30 - loss : 237.37827511154\n",
      "31 - loss : 237.31151242041005\n",
      "32 - loss : 237.2447793456047\n",
      "33 - loss : 237.1780758713053\n",
      "34 - loss : 237.11140198170366\n",
      "35 - loss : 237.0447576610021\n",
      "36 - loss : 236.97814289341335\n",
      "37 - loss : 236.91155766316052\n",
      "38 - loss : 236.84500195447762\n",
      "39 - loss : 236.77847575160857\n",
      "40 - loss : 236.7119790388081\n",
      "41 - loss : 236.64551180034127\n",
      "42 - loss : 236.57907402048346\n",
      "43 - loss : 236.51266568352085\n",
      "44 - loss : 236.4462867737496\n",
      "45 - loss : 236.37993727547646\n",
      "46 - loss : 236.31361717301868\n",
      "47 - loss : 236.24732645070372\n",
      "48 - loss : 236.18106509286955\n",
      "49 - loss : 236.11483308386445\n",
      "50 - loss : 236.04863040804713\n",
      "51 - loss : 235.98245704978655\n",
      "52 - loss : 235.91631299346218\n",
      "53 - loss : 235.8501982234637\n",
      "54 - loss : 235.7841127241912\n",
      "55 - loss : 235.7180564800551\n",
      "56 - loss : 235.65202947547593\n",
      "57 - loss : 235.5860316948849\n",
      "58 - loss : 235.52006312272317\n",
      "59 - loss : 235.4541237434425\n",
      "60 - loss : 235.38821354150468\n",
      "61 - loss : 235.32233250138194\n",
      "62 - loss : 235.2564806075566\n",
      "63 - loss : 235.1906578445215\n",
      "64 - loss : 235.12486419677955\n",
      "65 - loss : 235.05909964884387\n",
      "66 - loss : 234.99336418523797\n",
      "67 - loss : 234.92765779049552\n",
      "68 - loss : 234.86198044916034\n",
      "69 - loss : 234.79633214578647\n",
      "70 - loss : 234.73071286493834\n",
      "71 - loss : 234.66512259119028\n",
      "72 - loss : 234.59956130912713\n",
      "73 - loss : 234.5340290033435\n",
      "74 - loss : 234.46852565844472\n",
      "75 - loss : 234.40305125904567\n",
      "76 - loss : 234.3376057897719\n",
      "77 - loss : 234.2721892352588\n",
      "78 - loss : 234.20680158015207\n",
      "79 - loss : 234.1414428091074\n",
      "80 - loss : 234.0761129067905\n",
      "81 - loss : 234.01081185787766\n",
      "82 - loss : 233.9455396470549\n",
      "83 - loss : 233.8802962590182\n",
      "84 - loss : 233.81508167847406\n",
      "85 - loss : 233.7498958901388\n",
      "86 - loss : 233.68473887873873\n",
      "87 - loss : 233.61961062901048\n",
      "88 - loss : 233.5545111257006\n",
      "89 - loss : 233.48944035356558\n",
      "90 - loss : 233.4243982973722\n",
      "91 - loss : 233.35938494189708\n",
      "92 - loss : 233.2944002719269\n",
      "93 - loss : 233.22944427225843\n",
      "94 - loss : 233.16451692769837\n",
      "95 - loss : 233.09961822306352\n",
      "96 - loss : 233.0347481431806\n",
      "97 - loss : 232.96990667288625\n",
      "98 - loss : 232.9050937970272\n",
      "99 - loss : 232.84030950046005\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    z = torch.mm(x, w) + b\n",
    "    loss = torch.mean((y-z)**2)\n",
    "    \n",
    "    loss.backward()\n",
    "    w.data -= w.grad * lr\n",
    "    b.data -= b.grad * lr\n",
    "    \n",
    "    # grads = torch.autograd.grad(loss, [w, b])\n",
    "    # w.data -=grads[0] * lr\n",
    "    # b.data -=grads[1] * lr\n",
    "    \n",
    "    print(\"{} - loss : {}\".format(epoch, loss.item()), end=\"\\n\")\n",
    "    # 초기화\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD([w, b], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - loss : 232.77555376805168\n",
      "1 - loss : 232.71082658467827\n",
      "2 - loss : 232.64612793522662\n",
      "3 - loss : 232.5814578045931\n",
      "4 - loss : 232.51681617768423\n",
      "5 - loss : 232.4522030394161\n",
      "6 - loss : 232.3876183747151\n",
      "7 - loss : 232.32306216851723\n",
      "8 - loss : 232.25853440576873\n",
      "9 - loss : 232.19403507142525\n",
      "10 - loss : 232.12956415045278\n",
      "11 - loss : 232.06512162782704\n",
      "12 - loss : 232.0007074885334\n",
      "13 - loss : 231.9363217175674\n",
      "14 - loss : 231.87196429993418\n",
      "15 - loss : 231.80763522064908\n",
      "16 - loss : 231.74333446473676\n",
      "17 - loss : 231.67906201723216\n",
      "18 - loss : 231.6148178631798\n",
      "19 - loss : 231.55060198763408\n",
      "20 - loss : 231.48641437565917\n",
      "21 - loss : 231.4222550123291\n",
      "22 - loss : 231.35812388272774\n",
      "23 - loss : 231.2940209719485\n",
      "24 - loss : 231.2299462650948\n",
      "25 - loss : 231.16589974727972\n",
      "26 - loss : 231.10188140362624\n",
      "27 - loss : 231.03789121926687\n",
      "28 - loss : 230.97392917934394\n",
      "29 - loss : 230.9099952690096\n",
      "30 - loss : 230.8460894734257\n",
      "31 - loss : 230.78221177776362\n",
      "32 - loss : 230.7183621672049\n",
      "33 - loss : 230.65454062694013\n",
      "34 - loss : 230.59074714217022\n",
      "35 - loss : 230.5269816981054\n",
      "36 - loss : 230.46324427996578\n",
      "37 - loss : 230.3995348729809\n",
      "38 - loss : 230.33585346239028\n",
      "39 - loss : 230.27220003344283\n",
      "40 - loss : 230.20857457139724\n",
      "41 - loss : 230.1449770615218\n",
      "42 - loss : 230.08140748909454\n",
      "43 - loss : 230.01786583940284\n",
      "44 - loss : 229.9543520977441\n",
      "45 - loss : 229.89086624942496\n",
      "46 - loss : 229.82740827976198\n",
      "47 - loss : 229.7639781740809\n",
      "48 - loss : 229.70057591771763\n",
      "49 - loss : 229.63720149601716\n",
      "50 - loss : 229.57385489433423\n",
      "51 - loss : 229.51053609803324\n",
      "52 - loss : 229.44724509248798\n",
      "53 - loss : 229.3839818630818\n",
      "54 - loss : 229.3207463952078\n",
      "55 - loss : 229.2575386742684\n",
      "56 - loss : 229.1943586856757\n",
      "57 - loss : 229.13120641485128\n",
      "58 - loss : 229.0680818472261\n",
      "59 - loss : 229.00498496824065\n",
      "60 - loss : 228.94191576334524\n",
      "61 - loss : 228.87887421799928\n",
      "62 - loss : 228.8158603176717\n",
      "63 - loss : 228.75287404784126\n",
      "64 - loss : 228.6899153939958\n",
      "65 - loss : 228.62698434163286\n",
      "66 - loss : 228.5640808762592\n",
      "67 - loss : 228.50120498339126\n",
      "68 - loss : 228.4383566485547\n",
      "69 - loss : 228.3755358572849\n",
      "70 - loss : 228.31274259512634\n",
      "71 - loss : 228.24997684763323\n",
      "72 - loss : 228.18723860036897\n",
      "73 - loss : 228.12452783890635\n",
      "74 - loss : 228.0618445488277\n",
      "75 - loss : 227.99918871572467\n",
      "76 - loss : 227.9365603251983\n",
      "77 - loss : 227.87395936285887\n",
      "78 - loss : 227.81138581432617\n",
      "79 - loss : 227.7488396652294\n",
      "80 - loss : 227.686320901207\n",
      "81 - loss : 227.62382950790672\n",
      "82 - loss : 227.56136547098575\n",
      "83 - loss : 227.49892877611052\n",
      "84 - loss : 227.43651940895683\n",
      "85 - loss : 227.37413735520977\n",
      "86 - loss : 227.3117826005637\n",
      "87 - loss : 227.24945513072245\n",
      "88 - loss : 227.18715493139885\n",
      "89 - loss : 227.12488198831534\n",
      "90 - loss : 227.0626362872033\n",
      "91 - loss : 227.0004178138036\n",
      "92 - loss : 226.9382265538664\n",
      "93 - loss : 226.87606249315098\n",
      "94 - loss : 226.813925617426\n",
      "95 - loss : 226.75181591246914\n",
      "96 - loss : 226.68973336406748\n",
      "97 - loss : 226.6276779580174\n",
      "98 - loss : 226.5656496801243\n",
      "99 - loss : 226.5036485162029\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    z = x.matmul(w) + b\n",
    "    loss = torch.mean((z - y)**2)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()    \n",
    "    print(\"{} - loss : {}\".format(epoch, loss.item()), end=\"\\n\")\n",
    "    # 초기화\n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
